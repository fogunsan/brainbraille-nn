{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from https://github.com/yhzhao343/brainbraille_decode\n",
    "# Author: Yuhui Zhao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1726541232002,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "nJ5jSZ-DDmI_"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     # Dirty trick to format the file nicely\n",
    "#     from google.colab import drive\n",
    "\n",
    "#     !pip install black[jupyter] --quiet\n",
    "\n",
    "#     drive.mount(\"/content/drive\")\n",
    "#     !black /content/drive/MyDrive/brainbraille_new_pipeline.ipynb\n",
    "# except:\n",
    "#     pass\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install git+https://github.com/yhzhao343/brainbraille_decode.git\n",
    "    !pip install onedrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50565,
     "status": "ok",
     "timestamp": 1726541282572,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "NuNphmG_ZGmX",
    "outputId": "7711114a-1cb9-4d3b-c9ab-425f7055881f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! it takes FOREVER to import brainbraille_decode for the first time\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/_dlib_pybind11.cpython-311-darwin.so, 0x0002): tried: '/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/_dlib_pybind11.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/_dlib_pybind11.cpython-311-darwin.so' (no such file), '/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/_dlib_pybind11.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOTE! it takes FOREVER to import brainbraille_decode for the first time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbrainbraille_decode\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrainbraille_decode\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     grammar_info_gen,\n\u001b[1;32m     35\u001b[0m     letter_label,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     viterbi_decode,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrainbraille_decode\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     ROIandCalibrationExtractor,\n\u001b[1;32m     50\u001b[0m     ButterworthBandpassFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# DataSlice,\u001b[39;00m\n\u001b[1;32m     54\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/brainbraille_decode/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTK\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVM_Viterbi\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/brainbraille_decode/HTK.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[1;32m     14\u001b[0m rng \u001b[38;5;241m=\u001b[39m default_rng(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparseLatticeString\u001b[39m(latticeString):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/dlib/__init__.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m     add_lib_to_dll_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     add_lib_to_dll_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_CUDART_LIBRARY-NOTFOUND\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_dlib_pybind11\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_dlib_pybind11\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, __time_compiled__\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/_dlib_pybind11.cpython-311-darwin.so, 0x0002): tried: '/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/_dlib_pybind11.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/_dlib_pybind11.cpython-311-darwin.so' (no such file), '/opt/homebrew/anaconda3/envs/brainbrailleenv/lib/python3.11/site-packages/_dlib_pybind11.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import msgpack\n",
    "import msgpack_numpy as m\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import fastFMRI\n",
    "\n",
    "from fastFMRI.file_helpers import load_file, write_file\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from scipy.signal import butter, sosfilt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from fastFMRI.roi import (\n",
    "    get_roi_from_flatten_t,\n",
    "    get_calib_roi_from_flatten_roi,\n",
    "    get_aggregated_roi_from_flatten,\n",
    ")\n",
    "\n",
    "from fastFMRI.roi import extract_from_4d_data_path_using_3d_mask\n",
    "from joblib import Parallel, delayed\n",
    "print(\"NOTE! it takes FOREVER to import brainbraille_decode for the first time\")\n",
    "import brainbraille_decode\n",
    "\n",
    "from brainbraille_decode.lm import (\n",
    "    grammar_info_gen,\n",
    "    letter_label,\n",
    "    forward_decode,\n",
    "    viterbi_decode,\n",
    "    add_k_smoothing_1d,\n",
    "    add_k_smoothing_2d,\n",
    "    viterbi_decode_with_grammar_from_hidden_state_proba,\n",
    "    forward_decode_from_letter_proba_for_all_runs,\n",
    "    viterbi_decode_from_letter_proba_for_all_runs,\n",
    "    viterbi_decode_with_grammar_for_all_runs,\n",
    "    hidden_state_proba_to_emission_proba,\n",
    "    get_log_symbol_out_emission,\n",
    "    viterbi_decode,\n",
    ")\n",
    "from brainbraille_decode.preprocessing import (\n",
    "    ROIandCalibrationExtractor,\n",
    "    ButterworthBandpassFilter,\n",
    "    ZNormalizeByGroup,\n",
    "    LeadingTrailingDataSlice,\n",
    "    # DataSlice,\n",
    ")\n",
    "from brainbraille_decode.viterbi_decoder_helpers import (\n",
    "    get_symbol_node_trans,\n",
    "    letter_label_to_state_label,\n",
    "    clf_fit_pred_score,\n",
    "    param_result_format,\n",
    "    tune_clf_results_pretty_print,\n",
    "    get_slices_and_extract_data,\n",
    "    tune_clf,\n",
    "    clf_fit,\n",
    "    clf_predict_proba,\n",
    "    state_proba_to_letter_proba,\n",
    "    lipo_param_search,\n",
    "    tune_decode_smoothing,\n",
    "    clf_fit_each_r,\n",
    "    clf_predict_proba_each_r,\n",
    "    letter_bigram_viterbi_with_grammar_decode,\n",
    "    resolve_null,\n",
    "    symbol_info_preprocess,\n",
    "    joint_proba_of_hidden_state_sequence,\n",
    ")\n",
    "from brainbraille_decode.viterbi_decoder import StateProbaToLetterProb\n",
    "from sklearn.model_selection import (\n",
    "    LeaveOneOut,\n",
    "    LeaveOneGroupOut,\n",
    "    LeavePOut,\n",
    ")\n",
    "from brainbraille_decode.cross_validation import BrainBrailleCVGen\n",
    "from lipo import GlobalOptimizer\n",
    "from scipy.special import softmax\n",
    "from numba import jit, prange, f8, u4, i4, i8, b1\n",
    "import numba as nb\n",
    "\n",
    "m.patch()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1726541283376,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "d-CC07xuY1u4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2.08G/2.08G [11:18<00:00, 3.06MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# FILE_TO_USE = \"18S\"\n",
    "# FILE_TO_USE = \"6S\"\n",
    "FILE_TO_USE = \"3S\"\n",
    "#FILE_TO_USE = '1S5_THAD'\n",
    "# FILE_TO_USE = '1S5_FEYI'\n",
    "info_dict = {\n",
    "    \"18S\": {\n",
    "        \"ENV_VAR\": \"BRAINBRAILLE_INTERMEDIATE_18S_PER_RUN_RAW_DATA_FILE_PATH\",\n",
    "        \"file_name\": \"brainbraille_intermediate_12s+6s_run_raw_data.bin\",\n",
    "        \"url\": \"https://gtvault-my.sharepoint.com/:u:/g/personal/yzhao343_gatech_edu/EUK-mA0bnThOutq9_eOsJMIB0GUUn3DS7Tc8B-1UKRo7pA?e=HHrU5O\",\n",
    "    },\n",
    "    \"6S\": {\n",
    "        \"ENV_VAR\": \"BRAINBRAILLE_INTERMEDIATE_6S_PER_RUN_RAW_DATA_FILE_PATH\",\n",
    "        \"file_name\": \"brainbraille_intermediate_6s_run_raw_data.bin\",\n",
    "        \"url\": \"https://gtvault-my.sharepoint.com/:u:/g/personal/yzhao343_gatech_edu/EW56YHomjM1Egsqh43pncpEBK5iTG2dI0ccBDX9gZp07wQ?e=XzLuNN\",\n",
    "    },\n",
    "    \"3S\": {\n",
    "        \"ENV_VAR\": \"BRAINBRAILLE_INTERMEDIATE_3S_PER_RUN_RAW_DATA_FILE_PATH\",\n",
    "        \"file_name\": \"brainbraille_intermediate_3s_per_run_raw_data.bin\",\n",
    "        \"url\": \"https://gtvault-my.sharepoint.com/:u:/g/personal/yzhao343_gatech_edu/EYfSrpjQzcxKtVXkP1OeZrQBD6N1gNbiA3yqGjM6OhRoGg?e=Obj5Ho\",\n",
    "    },\n",
    "    \"1S5_THAD\": {\n",
    "        \"ENV_VAR\": \"BRAINBRAILLE_INTERMEDIATE_1S5_THAD_PER_RUN_RAW_DATA_FILE_PATH\",\n",
    "        \"file_name\": \"brainbraille_intermediate_thad_1s5_run_raw_data.bin\",\n",
    "        \"url\": \"https://gtvault-my.sharepoint.com/:u:/g/personal/yzhao343_gatech_edu/EdR7OAnM9W5JjD2FcSxWYa4Bs9vtvU157Zug4QiKmHvTvQ?e=yUfaxv\",\n",
    "    },\n",
    "    \"1S5_FEYI\": {\n",
    "        \"ENV_VAR\": \"BRAINBRAILLE_INTERMEDIATE_1S5_FEYI_PER_RUN_RAW_DATA_FILE_PATH\",\n",
    "        \"file_name\": \"brainbraille_intermediate_feyi_1s5_run_raw_data.bin\",\n",
    "        \"url\": \"https://gtvault-my.sharepoint.com/:u:/g/personal/yzhao343_gatech_edu/EUMLUnsFKNNNhkxWtf6q5pkBvqDWYHM817GuBSRywYJZ8A?e=8jodqH\"\n",
    "    },\n",
    "}\n",
    "\n",
    "ENV_VAR = info_dict[FILE_TO_USE][\"ENV_VAR\"]\n",
    "file_name = info_dict[FILE_TO_USE][\"file_name\"]\n",
    "url = info_dict[FILE_TO_USE][\"url\"]\n",
    "\n",
    "if ENV_VAR in os.environ:\n",
    "    brainbraille_intermediate_per_run_raw_data_file_path = os.environ[ENV_VAR]\n",
    "else:\n",
    "    brainbraille_intermediate_per_run_raw_data_file_path = file_name\n",
    "    if not os.path.exists(brainbraille_intermediate_per_run_raw_data_file_path):\n",
    "        from onedrivedownloader import download\n",
    "\n",
    "        # If the link expires, let me know and I will renew the link\n",
    "        download(\n",
    "            url=url,\n",
    "            filename=brainbraille_intermediate_per_run_raw_data_file_path,\n",
    "        )\n",
    "brainbraille_per_run_data = msgpack.unpackb(\n",
    "    load_file(brainbraille_intermediate_per_run_raw_data_file_path, \"rb\"),\n",
    "    strict_map_key=False,\n",
    ")\n",
    "per_run_data = brainbraille_per_run_data[\"per_run_data\"]\n",
    "# grammar_info = brainbraille_per_run_data[\"grammar_info\"]\n",
    "LETTERS_TO_DOT = brainbraille_per_run_data[\"LETTERS_TO_DOT\"]\n",
    "if ' ' not in LETTERS_TO_DOT:\n",
    "    LETTERS_TO_DOT[' '] = {'b': 0, 'f_l': 0, 'f_r': 0, 'h_l': 0, 'h_r': 0, 't': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1726541283395,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "SjO3AXIBZqUm",
    "outputId": "1b35c979-6f91-458b-c912-46d18c4ec579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached response because an exception occurred:\n",
      "[Errno 2] No such file or directory: 'None/HParse'\n"
     ]
    }
   ],
   "source": [
    "RUN_SVM = True\n",
    "\n",
    "if FILE_TO_USE == \"18S\":\n",
    "    TRAILLING_TIME_S = 6\n",
    "    LEADING_TIME_S = 0\n",
    "    DELAY_S = 12\n",
    "else:\n",
    "    TRAILLING_TIME_S = 6\n",
    "    LEADING_TIME_S = 6\n",
    "    DELAY_S = 3\n",
    "\n",
    "SVM_n_calls = 256\n",
    "RUN_BANDPASS = True           #@param {type:\"boolean\"}\n",
    "if RUN_BANDPASS:\n",
    "    BANDPASS_LOW_CUT = 0.01      #@param {type:\"slider\", min:0.000005, max:0.1, step:0.000005}\n",
    "    BANDPASS_HIGH_CUT = 0.2       #@param {type:\"slider\", min:0.11, max:1, step:0.005}\n",
    "    BANDPASS_ORDER = 1            #@param {type:\"slider\", \"min\":1, max:6, step:1}\n",
    "Z_NORM = True                 #@param {type:\"boolean\"}\n",
    "# EXTRA_TIME_S = 3\n",
    "TR_s = per_run_data[0][\"TR_s\"]\n",
    "STIMULI_LABEL_INTERVAL_s = per_run_data[0][\"EVENT_INTERVAL_S\"]\n",
    "# EXTRA_FRAME = int(EXTRA_TIME_S / TR_s)\n",
    "DELAY_FRAME = int(DELAY_S / TR_s)\n",
    "SF_Hz = 1 / TR_s\n",
    "NUM_FRAME_PER_LABEL = int(STIMULI_LABEL_INTERVAL_s / TR_s)\n",
    "EVENT_LEN_S = per_run_data[0][\"EVENT_LEN_S\"]\n",
    "EVENT_INTERVAL_S = per_run_data[0][\"EVENT_INTERVAL_S\"]\n",
    "EVENT_LEN_FRAME = int(EVENT_LEN_S / TR_s)\n",
    "EVENT_INTERVAL_FRAME = int(EVENT_INTERVAL_S / TR_s)\n",
    "regressor_types = per_run_data[0][\"regressor_types\"]\n",
    "inner_n_jobs = -1\n",
    "letter_labels = [d_i[\"letter_label\"] for d_i in per_run_data]\n",
    "grammar_info = grammar_info_gen(letter_labels, EVENT_LEN_S, False)\n",
    "LETTERS_TO_DOT_array = np.array(\n",
    "    [[LETTERS_TO_DOT[l][r] for r in regressor_types] for l in letter_label], dtype=bool\n",
    ")\n",
    "\n",
    "TRAILLING_FRAME = int(TRAILLING_TIME_S / TR_s)\n",
    "LEADING_FRAME = int(LEADING_TIME_S / TR_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1726541283412,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "ycDnES-7a7xa",
    "outputId": "4e2abc09-bbb9-4735-bd34-e12d3b658fc3"
   },
   "outputs": [],
   "source": [
    "print(letter_labels)\n",
    "print(f\"TR_s: {TR_s}s\\nSTIMULI_LABEL_INTERVAL_s: {STIMULI_LABEL_INTERVAL_s}s\\nEVENT_LEN_S: {EVENT_LEN_S}s\\nEVENT_INTERVAL_S: {EVENT_INTERVAL_S}s\\nEVENT_LEN_FRAME: {EVENT_LEN_FRAME}\\nEVENT_INTERVAL_FRAME: {EVENT_INTERVAL_FRAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1726541283428,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "OPoKyVsXaFGZ",
    "outputId": "759527c8-f0f9-4da2-d326-6b1cec2d0136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37]\n",
      "subs: [1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 4 4 4 4]\n",
      "runs: [1 2 3 4 5 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 1 2 1 2 3 4]\n",
      "sess: [1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "subs = np.array([info[\"sub\"] for info in per_run_data], dtype=int)\n",
    "runs = np.array([info[\"run\"] for info in per_run_data], dtype=int)\n",
    "sess = np.array([info[\"ses\"] for info in per_run_data], dtype=int)\n",
    "indx = np.arange(subs.size)\n",
    "print(f\"indx: {np.array2string(indx, max_line_width=120)}\")\n",
    "print(f\"subs: {np.array2string(subs, max_line_width=120)}\")\n",
    "print(f\"runs: {np.array2string(runs, max_line_width=120)}\")\n",
    "print(f\"sess: {np.array2string(sess, max_line_width=120)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1726541283439,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "SkxkNpT1hVRR",
    "outputId": "f87bc7f9-65ae-447b-ccb1-20d1a48e09cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 34 35 36 37]\n",
      "subs: [1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4]\n",
      "runs: [1 2 3 4 5 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 1 2 3 4]\n",
      "sess: [1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 1 1 1 1]\n",
      "fold: 0\n",
      "\ttrain (10): [ 1  2  3  4  5  6  7  8  9 10]\n",
      "\ttest  (01): [0]\n",
      "fold: 1\n",
      "\ttrain (10): [ 0  2  3  4  5  6  7  8  9 10]\n",
      "\ttest  (01): [1]\n",
      "fold: 2\n",
      "\ttrain (10): [ 0  1  3  4  5  6  7  8  9 10]\n",
      "\ttest  (01): [2]\n",
      "fold: 3\n",
      "\ttrain (10): [ 0  1  2  4  5  6  7  8  9 10]\n",
      "\ttest  (01): [3]\n",
      "fold: 4\n",
      "\ttrain (10): [ 0  1  2  3  5  6  7  8  9 10]\n",
      "\ttest  (01): [4]\n",
      "fold: 5\n",
      "\ttrain (10): [ 0  1  2  3  4  6  7  8  9 10]\n",
      "\ttest  (01): [5]\n",
      "fold: 6\n",
      "\ttrain (10): [ 0  1  2  3  4  5  7  8  9 10]\n",
      "\ttest  (01): [6]\n",
      "fold: 7\n",
      "\ttrain (10): [ 0  1  2  3  4  5  6  8  9 10]\n",
      "\ttest  (01): [7]\n",
      "fold: 8\n",
      "\ttrain (10): [ 0  1  2  3  4  5  6  7  9 10]\n",
      "\ttest  (01): [8]\n",
      "fold: 9\n",
      "\ttrain (10): [ 0  1  2  3  4  5  6  7  8 10]\n",
      "\ttest  (01): [9]\n",
      "fold: 10\n",
      "\ttrain (10): [0 1 2 3 4 5 6 7 8 9]\n",
      "\ttest  (01): [10]\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = True\n",
    "Z_NORM = True\n",
    "sub = 1\n",
    "# sub = 2\n",
    "n = 1\n",
    "test_sub = 1\n",
    "cv_sub_sampe_size = np.iinfo(np.int32).max\n",
    "# cv_sub_sampe_size = 5000\n",
    "sub_mask = (subs == 1) | (subs == 2) | (subs == 4)\n",
    "# sub_mask = (subs == 1) | (subs == 3)\n",
    "selected_ind = indx[sub_mask]\n",
    "selected_sub = subs[sub_mask]\n",
    "selected_run = runs[sub_mask]\n",
    "selected_ses = sess[sub_mask]\n",
    "\n",
    "print(f\"indx: {np.array2string(selected_ind, max_line_width=120)}\")\n",
    "print(f\"subs: {np.array2string(selected_sub, max_line_width=120)}\")\n",
    "print(f\"runs: {np.array2string(selected_run, max_line_width=120)}\")\n",
    "print(f\"sess: {np.array2string(selected_ses, max_line_width=120)}\")\n",
    "cv_generator = BrainBrailleCVGen(selected_sub, selected_run, selected_ses, selected_ind)\n",
    "\n",
    "per_run_state_label = letter_label_to_state_label(\n",
    "    [x_i[\"letter_label\"] for x_i in [per_run_data[i] for i in selected_ind]],\n",
    "    LETTERS_TO_DOT,\n",
    "    regressor_types,\n",
    ")\n",
    "for i, state_label in zip(selected_ind, per_run_state_label):\n",
    "    per_run_data[i][\"state_label\"] = state_label\n",
    "\n",
    "EXPERIMENT_TYPE=\"subj_dependent\"\n",
    "# EXPERIMENT_TYPE = \"subj_independent\"\n",
    "# EXPERIMENT_TYPE = \"subj_adaptive\"\n",
    "# EXPERIMENT_TYPE == \"subj_independent_LORO\"\n",
    "# EXPERIMENT_TYPE == \"subj_adaptive_LORO\"\n",
    "\n",
    "if EXPERIMENT_TYPE == \"subj_dependent\":\n",
    "    # # Subject dependent leave n run out:\n",
    "    train_test_split = cv_generator.sub_dependent_leave_n_run_out(sub, n)\n",
    "    train_vali_spliter = LeaveOneOut()\n",
    "elif EXPERIMENT_TYPE == \"subj_independent_LORO\":\n",
    "    train_test_split = cv_generator.sub_independent_leave_one_sub_out_test_sub(test_sub)\n",
    "    train_vali_spliter = LeaveOneOut()\n",
    "elif EXPERIMENT_TYPE == \"subj_adaptive_LORO\":\n",
    "    train_test_split = cv_generator.sub_adaptive_leave_one_sub_out_test_sub(test_sub)\n",
    "    train_vali_spliter = LeaveOneOut()\n",
    "elif EXPERIMENT_TYPE == \"subj_independent\":\n",
    "    # # Subject independent:\n",
    "    train_test_split = cv_generator.sub_independent_leave_one_sub_out_test_sub(test_sub)\n",
    "    train_train_i_list_all_fold, train_valid_i_list_all_fold = cv_generator.sub_independent_leave_one_sub_out_test_sub_train_valid(test_sub)\n",
    "    if \"train_vali_spliter\" in globals():\n",
    "        del train_vali_spliter\n",
    "elif EXPERIMENT_TYPE == \"subj_adaptive\":\n",
    "    # # Subject adaptive:\n",
    "    train_test_split = cv_generator.sub_adaptive_leave_one_sub_out_test_sub(test_sub)\n",
    "    train_train_i_list_all_fold, train_valid_i_list_all_fold = cv_generator.sub_adaptive_leave_one_sub_out_test_sub_train_valid(test_sub)\n",
    "    if \"train_vali_spliter\" in globals():\n",
    "        del train_vali_spliter\n",
    "\n",
    "fit_roi_all_sub=False\n",
    "if type(train_test_split) is tuple:\n",
    "    for fold_i, (train_i, test_i) in enumerate(zip(*train_test_split)):\n",
    "        print(f\"fold: {fold_i}\\n\\ttrain ({len(train_i):02}): {train_i}\\n\\ttest  ({len(test_i):02}): {test_i}\")\n",
    "        train_i_sub_set = set(subs[train_i])\n",
    "        test_i_sub_set = set(subs[test_i])\n",
    "        if not fit_roi_all_sub :\n",
    "            if (len(test_i_sub_set - train_i_sub_set) > 0):\n",
    "                fit_roi_all_sub = True\n",
    "\n",
    "if \"train_train_i_list_all_fold\" in globals():\n",
    "    print(\"=== example train valid  ===\")\n",
    "    for train_train_i, train_valid_i in zip(train_train_i_list_all_fold[0], train_valid_i_list_all_fold[0]):\n",
    "        print(train_train_i)\n",
    "        print(train_valid_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "S7I0H1_HlGV_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if RUN_SVM:\n",
    "    test_letter_label_list = []\n",
    "    test_state_label_list = []\n",
    "\n",
    "    test_naive_prob_letter_pred_list = []\n",
    "\n",
    "    region_train_valid_state_label_list = []\n",
    "    region_train_valid_state_pred_list = []\n",
    "    region_test_state_pred_list = []\n",
    "\n",
    "    region_cv_score_train_list = []\n",
    "    region_cv_score_test_list = []\n",
    "\n",
    "    test_naive_prob_letter_test_acc_list = []\n",
    "\n",
    "    stimulus_forward_decode_letter_list = []\n",
    "    stimulus_forward_decode_train_acc_list = []\n",
    "    stimulus_forward_decode_test_acc_list = []\n",
    "\n",
    "    stimulus_viterbi_decode_letter_list = []\n",
    "    stimulus_viterbi_decode_train_acc_list = []\n",
    "    stimulus_viterbi_decode_test_acc_list = []\n",
    "\n",
    "    mackenzie_soukoreff_forward_decode_letter_list = []\n",
    "    mackenzie_soukoreff_forward_decode_train_acc_list = []\n",
    "    mackenzie_soukoreff_forward_decode_test_acc_list = []\n",
    "\n",
    "    mackenzie_soukoreff_viterbi_decode_letter_list = []\n",
    "    mackenzie_soukoreff_viterbi_decode_train_acc_list = []\n",
    "    mackenzie_soukoreff_viterbi_decode_test_acc_list = []\n",
    "\n",
    "    stimulus_pred_letter_viterbi_decode_letter_label_list = []\n",
    "    stimulus_pred_letter_viterbi_decode_train_acc_list = []\n",
    "    stimulus_pred_letter_viterbi_decode_test_acc_list = []\n",
    "\n",
    "    aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list = []\n",
    "    aw2aw_stimulus_pred_letter_viterbi_decode_train_acc_list = []\n",
    "    aw2aw_stimulus_pred_letter_viterbi_decode_test_acc_list = []\n",
    "\n",
    "    mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list = []\n",
    "    mackenzie_soukoreff_pred_letter_viterbi_decode_train_list = []\n",
    "    mackenzie_soukoreff_pred_letter_viterbi_decode_test_list = []\n",
    "\n",
    "    aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list = []\n",
    "    aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_train_list = []\n",
    "    aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_test_list = []\n",
    "\n",
    "    total_time_start_i = time.time()\n",
    "    if \"train_test_spliter\" in globals():\n",
    "        if type(train_test_spliter) is LeaveOneOut:\n",
    "            train_test_split = partial(train_test_spliter.split, X=selected_run)\n",
    "            train_test_iter = train_test_split()\n",
    "    elif type(train_test_split) is tuple:\n",
    "        train_test_iter = zip(*train_test_split)\n",
    "\n",
    "    for fold_i, (train_i, test_i) in enumerate(train_test_iter):\n",
    "        fold_i_start_time = time.time()\n",
    "\n",
    "        num_letter_label = len(per_run_data[indx[train_i[0]]][\"letter_label\"])\n",
    "\n",
    "        roi_and_calib_extractor = ROIandCalibrationExtractor(fit_roi_all_sub=fit_roi_all_sub)\n",
    "\n",
    "        data_slicer = LeadingTrailingDataSlice(\n",
    "            leading_frame = LEADING_FRAME,\n",
    "            event_len_frame = EVENT_LEN_FRAME,\n",
    "            trailling_frame = TRAILLING_FRAME,\n",
    "            delay_frame = DELAY_FRAME,\n",
    "            event_interval_frame = EVENT_INTERVAL_FRAME,\n",
    "            fill_moving_avg_len_frame = EVENT_LEN_FRAME,\n",
    "            num_slices=num_letter_label,\n",
    "            feature_mask=None,\n",
    "        )\n",
    "\n",
    "        steps = [(\"roi_and_calib_extractor\", roi_and_calib_extractor)]\n",
    "        if RUN_BANDPASS:\n",
    "            butter_filter = ButterworthBandpassFilter(\n",
    "                BANDPASS_LOW_CUT,\n",
    "                BANDPASS_HIGH_CUT,\n",
    "                SF_Hz,\n",
    "                BANDPASS_ORDER,\n",
    "            )\n",
    "            steps.append((\"bandpass\", butter_filter))\n",
    "        steps.append((\"sliding_window\", data_slicer))\n",
    "        if Z_NORM:\n",
    "            steps.append((\"z_norm\", ZNormalizeByGroup()))\n",
    "\n",
    "        roi_extract_and_filter = Pipeline(steps)\n",
    "\n",
    "        if \"train_vali_spliter\" in globals():\n",
    "            if (type(train_vali_spliter) is LeaveOneOut):\n",
    "                train_vali_split = partial(train_vali_spliter.split, X=train_i)\n",
    "                train_train_i_list, train_valid_i_list = zip(*train_vali_split())\n",
    "        else:\n",
    "            train_train_i_list = train_train_i_list_all_fold[fold_i]\n",
    "            train_valid_i_list = train_valid_i_list_all_fold[fold_i]\n",
    "            # train_vali_split\n",
    "        # break\n",
    "        (\n",
    "            train_letter_label_i,\n",
    "            test_letter_label_i,\n",
    "            train_state_label_i,\n",
    "            test_state_label_i,\n",
    "            train_train_extracted_flatten_Xs,\n",
    "            train_train_state_flatten_labels,\n",
    "            train_train_letter_flatten_labels,\n",
    "            train_train_state_flatten_labels_by_r,\n",
    "            train_valid_extracted_flatten_Xs,\n",
    "            train_valid_state_flatten_labels,\n",
    "            train_valid_letter_flatten_labels,\n",
    "            train_valid_state_flatten_labels_by_r,\n",
    "            test_sub_i,\n",
    "            train_sub_i,\n",
    "            train_state_flatten_label_i,\n",
    "            train_data_i,\n",
    "            test_data_i,\n",
    "        ) = get_slices_and_extract_data(\n",
    "            indx,\n",
    "            fold_i,\n",
    "            per_run_data,\n",
    "            subs,\n",
    "            train_i,\n",
    "            test_i,\n",
    "            train_train_i_list,\n",
    "            train_valid_i_list,\n",
    "            roi_extract_and_filter,\n",
    "            Z_NORM,\n",
    "            n_jobs = -1,\n",
    "        )\n",
    "        # test_naive_prob_letter_label_list += test_letter_label_i\n",
    "        test_letter_label_i_flatten = np.concatenate(test_letter_label_i)\n",
    "        test_letter_label_list += test_letter_label_i\n",
    "        test_state_label_list += test_state_label_i\n",
    "        train_valid_label_flatten = np.concatenate(train_valid_letter_flatten_labels)\n",
    "\n",
    "        train_valid_state_flatten_labels_by_r = [[np.array(run_i).astype(np.bool_) for run_i in fold_i] for fold_i in train_valid_state_flatten_labels_by_r]\n",
    "        train_valid_state_flatten_labels = [[np.array(run_i).astype(np.bool_) for run_i in fold_i] for fold_i in train_valid_state_flatten_labels]\n",
    "        train_train_state_flatten_labels_by_r = [[np.array(run_i).astype(np.bool_) for run_i in fold_i] for fold_i in train_train_state_flatten_labels_by_r]\n",
    "        train_train_state_flatten_labels = [[np.array(run_i).astype(np.bool_) for run_i in fold_i] for fold_i in train_train_state_flatten_labels]\n",
    "\n",
    "\n",
    "        clf = SVC(kernel=\"rbf\", cache_size=2000)\n",
    "        param_category_keys = []\n",
    "        svc_param = {\n",
    "            \"C\": [1.0, 1000.0],\n",
    "            \"gamma\": [0.001, 0.1],\n",
    "        }\n",
    "        log_args_keys = [\"C\", \"gamma\"]\n",
    "        flexible_bounds = {\"C\": [False, True], \"gamma\": [False, True]}\n",
    "\n",
    "        # n_calls = 256\n",
    "        cv_sub_sampe_ratio = min(\n",
    "            cv_sub_sampe_size / len(train_train_extracted_flatten_Xs[0]), 1.0\n",
    "        )\n",
    "        region_cv_clfs, region_cv_tune_clf_results, region_cv_tune_clf_history = zip(\n",
    "            *Parallel(n_jobs=-1)(\n",
    "                delayed(tune_clf)(\n",
    "                    train_train_extracted_flatten_Xs,\n",
    "                    train_train_state_flatten_labels_by_r[r_i],\n",
    "                    train_valid_extracted_flatten_Xs,\n",
    "                    train_valid_state_flatten_labels_by_r[r_i],\n",
    "                    deepcopy(clf),\n",
    "                    svc_param,\n",
    "                    param_category_keys,\n",
    "                    log_args_keys,\n",
    "                    flexible_bounds,\n",
    "                    optimum_val_max=1.0,\n",
    "                    maximize=True,\n",
    "                    flexible_bound_threshold=0.1,\n",
    "                    random_state=42,\n",
    "                    n_calls=SVM_n_calls,\n",
    "                    sub_sample_ratio=cv_sub_sampe_ratio,\n",
    "                )\n",
    "                for r_i in range(len(regressor_types))\n",
    "            )\n",
    "        )\n",
    "        if VERBOSE:\n",
    "            tune_clf_results_pretty_print(region_cv_tune_clf_results, regressor_types)\n",
    "            fold_i_clf_tune_finish_time = time.time()\n",
    "            print(\n",
    "                f\"clf tune time: {fold_i_clf_tune_finish_time - fold_i_start_time:.3f}\"\n",
    "            )\n",
    "        region_cv_params, region_cv_score, _ = zip(*region_cv_tune_clf_results)\n",
    "        region_cv_score_train_list.append(region_cv_score)\n",
    "        # clf.set_params(probability=True)\n",
    "        region_clfs = [\n",
    "            deepcopy(clf).set_params(**param, probability=True)\n",
    "            for r, param in zip(regressor_types, region_cv_params)\n",
    "        ]\n",
    "        if Z_NORM:\n",
    "            roi_extract_and_filter.set_params(\n",
    "                z_norm__test_group=test_sub_i, z_norm__train_group=train_sub_i\n",
    "            )\n",
    "        train_data_extracted_i = roi_extract_and_filter.fit_transform(train_data_i)\n",
    "        test_data_extracted_i = roi_extract_and_filter.transform(test_data_i)\n",
    "\n",
    "        num_train_run, num_train_trial, num_train_frame, num_train_roi = (\n",
    "            train_data_extracted_i.shape\n",
    "        )\n",
    "        num_test_run, num_test_trial, num_test_frame, num_test_roi = (\n",
    "            test_data_extracted_i.shape\n",
    "        )\n",
    "\n",
    "        train_data_flatten_i = train_data_extracted_i.reshape(\n",
    "            np.prod(train_data_extracted_i.shape[:2]),\n",
    "            np.prod(train_data_extracted_i.shape[2:]),\n",
    "        )\n",
    "        test_data_flatten_i = test_data_extracted_i.reshape(\n",
    "            np.prod(test_data_extracted_i.shape[:2]),\n",
    "            np.prod(test_data_extracted_i.shape[2:]),\n",
    "        )\n",
    "\n",
    "        region_clfs = Parallel(n_jobs=-1)(\n",
    "            delayed(clf_fit)(\n",
    "                clf,\n",
    "                train_data_flatten_i,\n",
    "                [run_i[r_i] for run_i in train_state_flatten_label_i],\n",
    "            )\n",
    "            for r_i, clf in enumerate(region_clfs)\n",
    "        )\n",
    "        clf_train_end_time = time.time()\n",
    "        print(f\"clf train time: {clf_train_end_time - fold_i_clf_tune_finish_time:.3f}\")\n",
    "\n",
    "        test_predict_state = np.array(\n",
    "            Parallel(n_jobs=-1)(\n",
    "                delayed(clf_predict_proba)(clf, test_data_flatten_i)\n",
    "                for r_i, clf in enumerate(region_clfs)\n",
    "            )\n",
    "        )\n",
    "        test_predict_state = test_predict_state[:, :, 1].squeeze().T\n",
    "        test_predict_state_per_run = test_predict_state.reshape(\n",
    "            (num_test_run, num_test_trial, num_test_roi)\n",
    "        )\n",
    "        clf_inference_end_time = time.time()\n",
    "        print(f\"clf inference time: {clf_inference_end_time - clf_train_end_time:.3f}\")\n",
    "\n",
    "        (\n",
    "            test_naive_letter_proba_marg_per_run,\n",
    "            test_naive_letter_proba_ind_per_run,\n",
    "        ) = zip(\n",
    "            *[\n",
    "                state_proba_to_letter_proba(\n",
    "                    np.ascontiguousarray(run_i), LETTERS_TO_DOT_array\n",
    "                )\n",
    "                for run_i in test_predict_state_per_run\n",
    "            ]\n",
    "        )\n",
    "        test_naive_letter_prob_letter_per_run = [\n",
    "            [letter_label[l_i] for l_i in run_i]\n",
    "            for run_i in test_naive_letter_proba_ind_per_run\n",
    "        ]\n",
    "        region_test_state_pred_list.append(test_predict_state_per_run > 0.5)\n",
    "\n",
    "        test_naive_prob_letter_pred_list += test_naive_letter_prob_letter_per_run\n",
    "        # print(test_letter_label_i_flatten)\n",
    "        # print(np.concatenate(test_naive_letter_prob_letter_per_run))\n",
    "        test_naive_prob_letter_test_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(test_naive_letter_prob_letter_per_run),\n",
    "        )\n",
    "        test_naive_prob_letter_test_acc_list.append(test_naive_prob_letter_test_accuracy)\n",
    "\n",
    "        clf_train_inf_on_cv_split_start_time = time.time()\n",
    "        region_clfs_per_train_valid_split = Parallel(n_jobs=-1)(\n",
    "            delayed(clf_fit_each_r)(region_clfs, train_train_x, train_train_yr)\n",
    "            for train_train_x, train_train_yr in zip(\n",
    "                train_train_extracted_flatten_Xs, train_train_state_flatten_labels\n",
    "            )\n",
    "        )\n",
    "\n",
    "        train_valid_state_per_run = Parallel(n_jobs=-1)(\n",
    "            delayed(clf_predict_proba_each_r)(clfs, x_i)\n",
    "            for clfs, x_i in zip(\n",
    "                region_clfs_per_train_valid_split, train_valid_extracted_flatten_Xs\n",
    "            )\n",
    "        )\n",
    "        train_valid_state_per_run = [x_i[:, :, 1].T for x_i in train_valid_state_per_run]\n",
    "\n",
    "        (\n",
    "            train_valid_letter_proba_marg_per_run,\n",
    "            train_valid_letter_proba_ind_per_run,\n",
    "        ) = zip(\n",
    "            *[\n",
    "                state_proba_to_letter_proba(np.ascontiguousarray(run_i), LETTERS_TO_DOT_array)\n",
    "                for run_i in train_valid_state_per_run\n",
    "            ]\n",
    "        )\n",
    "        train_valid_letter_prob_letter_per_run = np.concatenate([\n",
    "            [letter_label[l_i] for l_i in run_i]\n",
    "            for run_i in train_valid_letter_proba_ind_per_run\n",
    "        ])\n",
    "        clf_train_inf_on_cv_split_end_time = time.time()\n",
    "        naive_prob_letter_train_accuracy = accuracy_score(\n",
    "            train_valid_label_flatten,\n",
    "            train_valid_letter_prob_letter_per_run,\n",
    "        )\n",
    "\n",
    "        train_valid_state_per_run = [run_i > 0.5 for run_i in train_valid_state_per_run]\n",
    "\n",
    "\n",
    "        test_acc_per_r = [\n",
    "            accuracy_score(\n",
    "                np.concatenate([[l_i[i] for l_i in fold_i] for fold_i in train_valid_state_flatten_labels]),\n",
    "                np.concatenate([fold_i[:, i] for fold_i in train_valid_state_per_run]),\n",
    "            )\n",
    "            for i in range(len(regressor_types))\n",
    "        ]\n",
    "        region_train_valid_state_label_list.append(train_valid_state_per_run)\n",
    "        region_train_valid_state_pred_list.append(train_valid_state_flatten_labels)\n",
    "\n",
    "        print(\"----- region train valid results -----\")\n",
    "        for i, r in enumerate(regressor_types):\n",
    "            print(f\"region:{r:>4} acc: {test_acc_per_r[i]:7.4f}\")\n",
    "        print(\"----- ------------------- ----\")\n",
    "        region_cv_score_test_list.append(test_acc_per_r)\n",
    "\n",
    "        print(\n",
    "            f\"Naive prob letter accuracy: train:{naive_prob_letter_train_accuracy:.4f} test:{test_naive_prob_letter_test_accuracy:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"clf train-inference on cv split time: {clf_train_inf_on_cv_split_end_time - clf_train_inf_on_cv_split_start_time:.3f}\"\n",
    "        )\n",
    "        if FILE_TO_USE in [\"6S\", \"18S\"]:\n",
    "            continue\n",
    "\n",
    "        initial_proba = np.zeros(len(letter_label), dtype=np.float64)\n",
    "        initial_proba[0] = 1.0\n",
    "        param_category_keys = []\n",
    "        log_args_keys = [\"uni_k\", \"bi_k\"]\n",
    "        # log_args_keys = []\n",
    "        decode_smoothing_param = {\n",
    "            \"uni_k\": [1e-3, 1e3],\n",
    "            \"bi_k\": [1e-3, 1e3],\n",
    "            \"ip_k\": [1e-3, 1e3],\n",
    "        }\n",
    "        flexible_bounds = {\n",
    "            \"uni_k\": [False, True],\n",
    "            \"bi_k\": [False, True],\n",
    "            \"ip_k\": [False, True],\n",
    "        }\n",
    "        n_calls = 1024\n",
    "        smoothing_1d = add_k_smoothing_1d\n",
    "        smoothing_2d = add_k_smoothing_2d\n",
    "\n",
    "        corpus = \"stimulus\"\n",
    "        print(f\"Letter decode using corpus: {corpus}\")\n",
    "        uni_gram_count = np.array(\n",
    "            grammar_info[f\"{corpus}_letter_one_gram_count\"], dtype=np.float64\n",
    "        )\n",
    "        bigram_gram_count = np.array(\n",
    "            grammar_info[f\"{corpus}_letter_bigram_count\"], dtype=np.float64\n",
    "        )\n",
    "\n",
    "        (\n",
    "            optimized_forward_decoder,\n",
    "            optimized_forward_decoder_results,\n",
    "            optimized_forward_decoder_history,\n",
    "        ) = tune_decode_smoothing(\n",
    "            train_valid_letter_proba_marg_per_run,\n",
    "            train_valid_letter_flatten_labels,\n",
    "            uni_gram_count,\n",
    "            bigram_gram_count,\n",
    "            initial_proba,\n",
    "            letter_label,\n",
    "            forward_decode_from_letter_proba_for_all_runs,\n",
    "            smoothing_1d,\n",
    "            smoothing_2d,\n",
    "            decode_smoothing_param,\n",
    "            param_category_keys,\n",
    "            log_args_keys,\n",
    "            flexible_bounds,\n",
    "            n_calls=n_calls,\n",
    "        )\n",
    "\n",
    "        test_forward_decode_letter_per_run = optimized_forward_decoder(\n",
    "            letter_proba_per_run=test_naive_letter_proba_marg_per_run\n",
    "        )\n",
    "        stimulus_forward_decode_letter_list += test_forward_decode_letter_per_run\n",
    "        stim_forward_letter_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(test_forward_decode_letter_per_run),\n",
    "        )\n",
    "        stim_forward_decode_train_acc = optimized_forward_decoder_results[1]\n",
    "        stimulus_forward_decode_train_acc_list.append(stim_forward_decode_train_acc)\n",
    "        stimulus_forward_decode_test_acc_list.append(stim_forward_letter_accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"\\tForward decode: train:{stim_forward_decode_train_acc:.4f} test:{stim_forward_letter_accuracy:.4f} {param_result_format(optimized_forward_decoder_results[0])}\"\n",
    "        )\n",
    "\n",
    "        (\n",
    "            optimized_viterbi_decoder,\n",
    "            optimized_viterbi_decoder_results,\n",
    "            optimized_viterbi_decoder_history,\n",
    "        ) = tune_decode_smoothing(\n",
    "            train_valid_letter_proba_marg_per_run,\n",
    "            train_valid_letter_flatten_labels,\n",
    "            uni_gram_count,\n",
    "            bigram_gram_count,\n",
    "            initial_proba,\n",
    "            letter_label,\n",
    "            viterbi_decode_from_letter_proba_for_all_runs,\n",
    "            smoothing_1d,\n",
    "            smoothing_2d,\n",
    "            decode_smoothing_param,\n",
    "            param_category_keys,\n",
    "            log_args_keys,\n",
    "            flexible_bounds,\n",
    "            n_calls=n_calls,\n",
    "        )\n",
    "\n",
    "        test_viterbi_decode_letter_per_run = optimized_viterbi_decoder(\n",
    "            letter_proba_per_run=test_naive_letter_proba_marg_per_run\n",
    "        )\n",
    "        stimulus_viterbi_decode_letter_list += test_viterbi_decode_letter_per_run\n",
    "        stim_viterbi_letter_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(test_viterbi_decode_letter_per_run),\n",
    "        )\n",
    "        stim_viterbi_decode_train_acc = optimized_viterbi_decoder_results[1]\n",
    "        stimulus_viterbi_decode_train_acc_list.append(stim_viterbi_decode_train_acc)\n",
    "        stimulus_viterbi_decode_test_acc_list.append(stim_viterbi_letter_accuracy)\n",
    "        print(\n",
    "            f\"\\tViterbi decode: train:{stim_viterbi_decode_train_acc:.4f} test:{stim_viterbi_letter_accuracy:.4f} {param_result_format(optimized_viterbi_decoder_results[0])}\"\n",
    "        )\n",
    "\n",
    "        corpus = \"mackenzie_soukoreff\"\n",
    "        print(f\"Letter decode using corpus: {corpus}\")\n",
    "        uni_gram_count = np.array(\n",
    "            grammar_info[f\"{corpus}_letter_one_gram_count\"], dtype=np.float64\n",
    "        )\n",
    "        bigram_gram_count = np.array(\n",
    "            grammar_info[f\"{corpus}_letter_bigram_count\"], dtype=np.float64\n",
    "        )\n",
    "\n",
    "        (\n",
    "            optimized_forward_decoder,\n",
    "            optimized_forward_decoder_results,\n",
    "            optimized_forward_decoder_history,\n",
    "        ) = tune_decode_smoothing(\n",
    "            train_valid_letter_proba_marg_per_run,\n",
    "            train_valid_letter_flatten_labels,\n",
    "            uni_gram_count,\n",
    "            bigram_gram_count,\n",
    "            initial_proba,\n",
    "            letter_label,\n",
    "            forward_decode_from_letter_proba_for_all_runs,\n",
    "            smoothing_1d,\n",
    "            smoothing_2d,\n",
    "            decode_smoothing_param,\n",
    "            param_category_keys,\n",
    "            log_args_keys,\n",
    "            flexible_bounds,\n",
    "            n_calls=n_calls,\n",
    "        )\n",
    "\n",
    "        test_forward_decode_letter_per_run = optimized_forward_decoder(\n",
    "            letter_proba_per_run=test_naive_letter_proba_marg_per_run\n",
    "        )\n",
    "        mackenzie_soukoreff_forward_decode_letter_list += (\n",
    "            test_forward_decode_letter_per_run\n",
    "        )\n",
    "        mackenzie_soukoreff_forward_letter_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(test_forward_decode_letter_per_run),\n",
    "        )\n",
    "        mackenzie_soukoreff_forward_decode_train_acc = (\n",
    "            optimized_forward_decoder_results[1]\n",
    "        )\n",
    "        mackenzie_soukoreff_forward_decode_train_acc_list.append(\n",
    "            mackenzie_soukoreff_forward_decode_train_acc\n",
    "        )\n",
    "        mackenzie_soukoreff_forward_decode_test_acc_list.append(\n",
    "            mackenzie_soukoreff_forward_letter_accuracy\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tForward decode: train:{mackenzie_soukoreff_forward_decode_train_acc:.4f} test:{mackenzie_soukoreff_forward_letter_accuracy:.4f} {param_result_format(optimized_forward_decoder_results[0])}\"\n",
    "        )\n",
    "\n",
    "        (\n",
    "            optimized_viterbi_decoder,\n",
    "            optimized_viterbi_decoder_results,\n",
    "            optimized_viterbi_decoder_history,\n",
    "        ) = tune_decode_smoothing(\n",
    "            train_valid_letter_proba_marg_per_run,\n",
    "            train_valid_letter_flatten_labels,\n",
    "            uni_gram_count,\n",
    "            bigram_gram_count,\n",
    "            initial_proba,\n",
    "            letter_label,\n",
    "            viterbi_decode_from_letter_proba_for_all_runs,\n",
    "            smoothing_1d,\n",
    "            smoothing_2d,\n",
    "            decode_smoothing_param,\n",
    "            param_category_keys,\n",
    "            log_args_keys,\n",
    "            flexible_bounds,\n",
    "            n_calls=n_calls,\n",
    "        )\n",
    "\n",
    "        test_viterbi_decode_letter_per_run = optimized_viterbi_decoder(\n",
    "            letter_proba_per_run=test_naive_letter_proba_marg_per_run\n",
    "        )\n",
    "        mackenzie_soukoreff_viterbi_decode_letter_list += (\n",
    "            test_viterbi_decode_letter_per_run\n",
    "        )\n",
    "        mackenzie_soukoreff_viterbi_letter_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(test_viterbi_decode_letter_per_run),\n",
    "        )\n",
    "        mackenzie_soukoreff_viterbi_decode_train_acc = (\n",
    "            optimized_viterbi_decoder_results[1]\n",
    "        )\n",
    "        mackenzie_soukoreff_viterbi_decode_train_acc_list.append(\n",
    "            mackenzie_soukoreff_viterbi_decode_train_acc\n",
    "        )\n",
    "        mackenzie_soukoreff_viterbi_decode_test_acc_list.append(\n",
    "            mackenzie_soukoreff_viterbi_letter_accuracy\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"\\tViterbi decode: train:{mackenzie_soukoreff_viterbi_decode_train_acc:.4f} test:{mackenzie_soukoreff_viterbi_letter_accuracy:.4f} {param_result_format(optimized_viterbi_decoder_results[0])}\"\n",
    "        )\n",
    "\n",
    "        grammar_info = grammar_info_gen(letter_labels, EVENT_LEN_S, False)\n",
    "        corpus = \"stimulus\"\n",
    "        print(f\"Grammar decode using corpus: {corpus}\")\n",
    "        initial_proba = np.zeros(len(letter_label), dtype=np.float64)\n",
    "        initial_proba[0] = 1.0\n",
    "        param_category_keys = []\n",
    "        log_args_keys = [\"uni_k\", \"bi_k\"]\n",
    "        log_args_keys = []\n",
    "        decode_smoothing_param = {\n",
    "            \"uni_k\": [1e-3, 1e3],\n",
    "            \"bi_k\": [1e-3, 1e3],\n",
    "            # \"ip_k\":  [0.001/27, 1.0/27],\n",
    "            # \"insert_panelty\": [-10, 10],\n",
    "        }\n",
    "        flexible_bounds = {\n",
    "            \"uni_k\": [False, True],\n",
    "            \"bi_k\": [False, True],\n",
    "            # \"ip_k\": [False, True],\n",
    "            # \"insert_panelty\": [True, True],\n",
    "        }\n",
    "        n_calls = 1024\n",
    "\n",
    "        # ===== stimulus - SFFW =====\n",
    "        prefix = \"\"\n",
    "        symbol_nodes = grammar_info[f\"{prefix}{corpus}_words_node_symbols\"]\n",
    "        symbol_node_edges = grammar_info[f\"{prefix}{corpus}_words_link_start_end\"]\n",
    "        node_output_letter = grammar_info[f\"unique_{corpus}_word_dictionary\"]\n",
    "\n",
    "        (\n",
    "            symbol_node_trans,\n",
    "            symbol_nodes_spelling_ndx,\n",
    "            symbol_nodes_out_spelling_matrix,\n",
    "            symbol_nodes_out_len,\n",
    "            dummy_start_node_i,\n",
    "            end_node_index,\n",
    "        ) = symbol_info_preprocess(\n",
    "            letter_label, symbol_nodes, symbol_node_edges, node_output_letter\n",
    "        )\n",
    "\n",
    "        viterbi_decode_with_grammar_for_all_runs_with_meta_data = partial(\n",
    "            viterbi_decode_with_grammar_for_all_runs,\n",
    "            dummy_start_node_i=dummy_start_node_i,\n",
    "            end_node_index=end_node_index,\n",
    "            symbol_node_trans=symbol_node_trans,\n",
    "            symbol_nodes_spelling_ndx=symbol_nodes_spelling_ndx,\n",
    "            symbol_nodes_out_spelling_matrix=symbol_nodes_out_spelling_matrix,\n",
    "            symbol_nodes_out_len=symbol_nodes_out_len,\n",
    "        )\n",
    "        (\n",
    "            optimized_viterbi_decode_with_grammar,\n",
    "            optimized_viterbi_decode_with_grammar_results,\n",
    "            optimized_viterbi_decode_with_grammar_history,\n",
    "        ) = tune_decode_smoothing(\n",
    "            train_valid_letter_proba_marg_per_run,\n",
    "            train_valid_letter_flatten_labels,\n",
    "            uni_gram_count,\n",
    "            bigram_gram_count,\n",
    "            initial_proba,\n",
    "            letter_label,\n",
    "            viterbi_decode_with_grammar_for_all_runs_with_meta_data,\n",
    "            smoothing_1d,\n",
    "            smoothing_2d,\n",
    "            decode_smoothing_param,\n",
    "            param_category_keys,\n",
    "            log_args_keys,\n",
    "            flexible_bounds,\n",
    "            n_calls=n_calls,\n",
    "        )\n",
    "\n",
    "        test_stim_viterbi_decode_with_grammar_per_run = (\n",
    "            optimized_viterbi_decode_with_grammar(\n",
    "                letter_proba_per_run=test_naive_letter_proba_marg_per_run\n",
    "            )\n",
    "        )\n",
    "        stimulus_pred_letter_viterbi_decode_letter_label_list += (\n",
    "            test_stim_viterbi_decode_with_grammar_per_run\n",
    "        )\n",
    "        stim_viterbi_decode_with_grammar_letter_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(test_stim_viterbi_decode_with_grammar_per_run),\n",
    "        )\n",
    "        stimulus_pred_letter_viterbi_decode_train_acc = (\n",
    "            optimized_viterbi_decode_with_grammar_results[1]\n",
    "        )\n",
    "        stimulus_pred_letter_viterbi_decode_train_acc_list.append(\n",
    "            stimulus_pred_letter_viterbi_decode_train_acc\n",
    "        )\n",
    "        stimulus_pred_letter_viterbi_decode_test_acc_list.append(\n",
    "            stim_viterbi_decode_with_grammar_letter_accuracy\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tSFFW  viterbi decode: train:{stimulus_pred_letter_viterbi_decode_train_acc:.4f} test:{stim_viterbi_decode_with_grammar_letter_accuracy:.4f} {param_result_format(optimized_viterbi_decode_with_grammar_results[0])}\"\n",
    "        )\n",
    "\n",
    "        # ===== stimulus - aw2aw =====\n",
    "        prefix = \"aw2aw_\"\n",
    "        symbol_nodes = grammar_info[f\"{prefix}{corpus}_words_node_symbols\"]\n",
    "        symbol_node_edges = grammar_info[f\"{prefix}{corpus}_words_link_start_end\"]\n",
    "        node_output_letter = grammar_info[f\"unique_{corpus}_word_dictionary\"]\n",
    "\n",
    "        (\n",
    "            symbol_node_trans,\n",
    "            symbol_nodes_spelling_ndx,\n",
    "            symbol_nodes_out_spelling_matrix,\n",
    "            symbol_nodes_out_len,\n",
    "            dummy_start_node_i,\n",
    "            end_node_index,\n",
    "        ) = symbol_info_preprocess(\n",
    "            letter_label, symbol_nodes, symbol_node_edges, node_output_letter\n",
    "        )\n",
    "\n",
    "        viterbi_decode_with_grammar_for_all_runs_with_meta_data = partial(\n",
    "            viterbi_decode_with_grammar_for_all_runs,\n",
    "            dummy_start_node_i=dummy_start_node_i,\n",
    "            end_node_index=end_node_index,\n",
    "            symbol_node_trans=symbol_node_trans,\n",
    "            symbol_nodes_spelling_ndx=symbol_nodes_spelling_ndx,\n",
    "            symbol_nodes_out_spelling_matrix=symbol_nodes_out_spelling_matrix,\n",
    "            symbol_nodes_out_len=symbol_nodes_out_len,\n",
    "        )\n",
    "        (\n",
    "            aw2aw_optimized_viterbi_decode_with_grammar,\n",
    "            aw2aw_optimized_viterbi_decode_with_grammar_results,\n",
    "            aw2aw_optimized_viterbi_decode_with_grammar_history,\n",
    "        ) = tune_decode_smoothing(\n",
    "            train_valid_letter_proba_marg_per_run,\n",
    "            train_valid_letter_flatten_labels,\n",
    "            uni_gram_count,\n",
    "            bigram_gram_count,\n",
    "            initial_proba,\n",
    "            letter_label,\n",
    "            viterbi_decode_with_grammar_for_all_runs_with_meta_data,\n",
    "            smoothing_1d,\n",
    "            smoothing_2d,\n",
    "            decode_smoothing_param,\n",
    "            param_category_keys,\n",
    "            log_args_keys,\n",
    "            flexible_bounds,\n",
    "            n_calls=n_calls,\n",
    "        )\n",
    "        aw2aw_test_stim_viterbi_decode_with_grammar_per_run = (\n",
    "            aw2aw_optimized_viterbi_decode_with_grammar(\n",
    "                letter_proba_per_run=test_naive_letter_proba_marg_per_run\n",
    "            )\n",
    "        )\n",
    "        aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list += (\n",
    "            aw2aw_test_stim_viterbi_decode_with_grammar_per_run\n",
    "        )\n",
    "        aw2aw_stim_viterbi_decode_with_grammar_letter_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(aw2aw_test_stim_viterbi_decode_with_grammar_per_run),\n",
    "        )\n",
    "        aw2aw_stimulus_pred_letter_viterbi_decode_train_acc = (\n",
    "            aw2aw_optimized_viterbi_decode_with_grammar_results[1]\n",
    "        )\n",
    "        aw2aw_stimulus_pred_letter_viterbi_decode_train_acc_list.append(\n",
    "            aw2aw_stimulus_pred_letter_viterbi_decode_train_acc\n",
    "        )\n",
    "        aw2aw_stimulus_pred_letter_viterbi_decode_test_acc_list.append(\n",
    "            aw2aw_stim_viterbi_decode_with_grammar_letter_accuracy\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"\\taw2aw viterbi decode: train:{aw2aw_stimulus_pred_letter_viterbi_decode_train_acc:.4f} test:{aw2aw_stim_viterbi_decode_with_grammar_letter_accuracy:.4f} {param_result_format(aw2aw_optimized_viterbi_decode_with_grammar_results[0])}\"\n",
    "        )\n",
    "\n",
    "        # # ===== mackenzie_soukoreff - SFFW =====\n",
    "        corpus = \"mackenzie_soukoreff\"\n",
    "        print(f\"Grammar decode using corpus: {corpus}\")\n",
    "        prefix = \"\"\n",
    "        symbol_nodes = grammar_info[f\"{prefix}{corpus}_words_node_symbols\"]\n",
    "        symbol_node_edges = grammar_info[f\"{prefix}{corpus}_words_link_start_end\"]\n",
    "        node_output_letter = grammar_info[f\"unique_{corpus}_word_dictionary\"]\n",
    "\n",
    "        (\n",
    "            symbol_node_trans,\n",
    "            symbol_nodes_spelling_ndx,\n",
    "            symbol_nodes_out_spelling_matrix,\n",
    "            symbol_nodes_out_len,\n",
    "            dummy_start_node_i,\n",
    "            end_node_index,\n",
    "        ) = symbol_info_preprocess(\n",
    "            letter_label, symbol_nodes, symbol_node_edges, node_output_letter\n",
    "        )\n",
    "\n",
    "        mackenzie_soukoreff_optimized_viterbi_decode_with_grammar = partial(\n",
    "            viterbi_decode_with_grammar_for_all_runs,\n",
    "            dummy_start_node_i=dummy_start_node_i,\n",
    "            end_node_index=end_node_index,\n",
    "            symbol_node_trans=symbol_node_trans,\n",
    "            symbol_nodes_spelling_ndx=symbol_nodes_spelling_ndx,\n",
    "            symbol_nodes_out_spelling_matrix=symbol_nodes_out_spelling_matrix,\n",
    "            symbol_nodes_out_len=symbol_nodes_out_len,\n",
    "            uni_count=uni_gram_count,\n",
    "            bi_count=bigram_gram_count,\n",
    "            initial_proba=initial_proba,\n",
    "            **optimized_viterbi_decode_with_grammar_results[0],\n",
    "        )\n",
    "\n",
    "        test_stim_viterbi_decode_with_grammar_per_run = (\n",
    "            mackenzie_soukoreff_optimized_viterbi_decode_with_grammar(\n",
    "                letter_proba_per_run=test_naive_letter_proba_marg_per_run\n",
    "            )\n",
    "        )\n",
    "        stimulus_pred_letter_viterbi_decode_letter_label_list += (\n",
    "            test_stim_viterbi_decode_with_grammar_per_run\n",
    "        )\n",
    "        stim_viterbi_decode_with_grammar_letter_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(test_stim_viterbi_decode_with_grammar_per_run),\n",
    "        )\n",
    "        mackenzie_soukoreff_pred_letter_viterbi_decode_train = (\n",
    "            optimized_viterbi_decode_with_grammar_results[1]\n",
    "        )\n",
    "        mackenzie_soukoreff_pred_letter_viterbi_decode_train_list.append(\n",
    "            mackenzie_soukoreff_pred_letter_viterbi_decode_train\n",
    "        )\n",
    "        mackenzie_soukoreff_pred_letter_viterbi_decode_test_list.append(\n",
    "            stim_viterbi_decode_with_grammar_letter_accuracy\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"\\tSFFW  viterbi decode: train:{mackenzie_soukoreff_pred_letter_viterbi_decode_train:.4f} test:{stim_viterbi_decode_with_grammar_letter_accuracy:.4f} {param_result_format(optimized_viterbi_decode_with_grammar_results[0])}\"\n",
    "        )\n",
    "\n",
    "        # # ===== mackenzie_soukoreff - aw2aw =====\n",
    "        prefix = \"aw2aw_\"\n",
    "        symbol_nodes = grammar_info[f\"{prefix}{corpus}_words_node_symbols\"]\n",
    "        symbol_node_edges = grammar_info[f\"{prefix}{corpus}_words_link_start_end\"]\n",
    "        node_output_letter = grammar_info[f\"unique_{corpus}_word_dictionary\"]\n",
    "\n",
    "        (\n",
    "            symbol_node_trans,\n",
    "            symbol_nodes_spelling_ndx,\n",
    "            symbol_nodes_out_spelling_matrix,\n",
    "            symbol_nodes_out_len,\n",
    "            dummy_start_node_i,\n",
    "            end_node_index,\n",
    "        ) = symbol_info_preprocess(\n",
    "            letter_label, symbol_nodes, symbol_node_edges, node_output_letter\n",
    "        )\n",
    "\n",
    "        aw2aw_mackenzie_soukoreff_optimized_viterbi_decode_with_grammar = partial(\n",
    "            viterbi_decode_with_grammar_for_all_runs,\n",
    "            dummy_start_node_i=dummy_start_node_i,\n",
    "            end_node_index=end_node_index,\n",
    "            symbol_node_trans=symbol_node_trans,\n",
    "            symbol_nodes_spelling_ndx=symbol_nodes_spelling_ndx,\n",
    "            symbol_nodes_out_spelling_matrix=symbol_nodes_out_spelling_matrix,\n",
    "            symbol_nodes_out_len=symbol_nodes_out_len,\n",
    "            uni_count=uni_gram_count,\n",
    "            bi_count=bigram_gram_count,\n",
    "            initial_proba=initial_proba,\n",
    "            **aw2aw_optimized_viterbi_decode_with_grammar_results[0],\n",
    "        )\n",
    "        aw2aw_test_stim_viterbi_decode_with_grammar_per_run = (\n",
    "            aw2aw_mackenzie_soukoreff_optimized_viterbi_decode_with_grammar(\n",
    "                letter_proba_per_run=test_naive_letter_proba_marg_per_run\n",
    "            )\n",
    "        )\n",
    "        aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list += (\n",
    "            aw2aw_test_stim_viterbi_decode_with_grammar_per_run\n",
    "        )\n",
    "        aw2aw_stim_viterbi_decode_with_grammar_letter_accuracy = accuracy_score(\n",
    "            test_letter_label_i_flatten,\n",
    "            np.concatenate(aw2aw_test_stim_viterbi_decode_with_grammar_per_run),\n",
    "        )\n",
    "        aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_train = (\n",
    "            aw2aw_optimized_viterbi_decode_with_grammar_results[1]\n",
    "        )\n",
    "        aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_train_list.append(\n",
    "            aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_train\n",
    "        )\n",
    "        aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_test_list.append(\n",
    "            aw2aw_stim_viterbi_decode_with_grammar_letter_accuracy\n",
    "        )\n",
    "        print(\n",
    "            f\"\\taw2aw viterbi decode: train:{aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_train:.4f} test:{aw2aw_stim_viterbi_decode_with_grammar_letter_accuracy:.4f} {param_result_format(aw2aw_optimized_viterbi_decode_with_grammar_results[0])}\"\n",
    "        )\n",
    "\n",
    "        fold_i_end_time = time.time()\n",
    "        print(f\"{fold_i_end_time - fold_i_start_time:.4f}s\")\n",
    "        # break\n",
    "\n",
    "    total_time_end_i = time.time()\n",
    "    print(f\"total: {total_time_end_i - total_time_start_i:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 614306,
     "status": "aborted",
     "timestamp": 1726541846243,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "1YWkm4cBS0RJ"
   },
   "outputs": [],
   "source": [
    "# test_state_label_list = np.array(test_state_label_list)\n",
    "region_test_state_pred_list_concat = np.concatenate(region_test_state_pred_list)\n",
    "test_state_label_list_flatten = np.concatenate(test_state_label_list)\n",
    "region_test_state_pred_list_flatten = np.concatenate(region_test_state_pred_list_concat)\n",
    "print(\"----- region train test results -----\")\n",
    "for i, r in enumerate(regressor_types):\n",
    "    print(f\"region:{r:>4} acc: {accuracy_score(test_state_label_list_flatten[:,i], region_test_state_pred_list_flatten[:, i]):7.4f}\")\n",
    "print(\"----- ------------------- ----\")\n",
    "\n",
    "naive_accuracy = accuracy_score(\n",
    "    [j for i in test_letter_label_list for j in i],\n",
    "    [j for i in test_naive_prob_letter_pred_list for j in i],\n",
    ")\n",
    "print(f\"naive accuracy: {naive_accuracy:.4f}\")\n",
    "\n",
    "if FILE_TO_USE not in [\"6S\", \"18S\"]:\n",
    "\n",
    "    stim_forward_decode_accuracy = accuracy_score(\n",
    "        [j for i in test_letter_label_list for j in i],\n",
    "        [j for i in stimulus_forward_decode_letter_list for j in i],\n",
    "    )\n",
    "\n",
    "    stim_viterbi_decode_accuracy = accuracy_score(\n",
    "        [j for i in test_letter_label_list for j in i],\n",
    "        [j for i in stimulus_viterbi_decode_letter_list for j in i],\n",
    "    )\n",
    "\n",
    "    mackenzie_soukoreff_forward_decode_accuracy = accuracy_score(\n",
    "        [j for i in test_letter_label_list for j in i],\n",
    "        [j for i in mackenzie_soukoreff_forward_decode_letter_list for j in i],\n",
    "    )\n",
    "\n",
    "    mackenzie_soukoreff_viterbi_decode_accuracy = accuracy_score(\n",
    "        [j for i in test_letter_label_list for j in i],\n",
    "        [j for i in mackenzie_soukoreff_viterbi_decode_letter_list for j in i],\n",
    "    )\n",
    "\n",
    "    sffw_stim_grammar_acc = np.mean(stimulus_pred_letter_viterbi_decode_test_acc_list)\n",
    "    aw2aw_stim_grammar_acc = np.mean(\n",
    "        aw2aw_stimulus_pred_letter_viterbi_decode_test_acc_list\n",
    "    )\n",
    "    sffw_machenzie_soukoreff_grammar_acc = np.mean(\n",
    "        mackenzie_soukoreff_pred_letter_viterbi_decode_test_list\n",
    "    )\n",
    "    aw2aw_machenzie_soukoreff_grammar_acc = np.mean(\n",
    "        aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_test_list\n",
    "    )\n",
    "\n",
    "    print(\"\\nletter decoding:\")\n",
    "    print(\n",
    "        f\"stim\\n\\tforward acc:{stim_forward_decode_accuracy:.4f}\\n\\tviterbi acc:{stim_viterbi_decode_accuracy:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"mackenzie_soukoreff\\n\\tforward acc:{mackenzie_soukoreff_forward_decode_accuracy:.4f}\\n\\tviterbi acc:{mackenzie_soukoreff_viterbi_decode_accuracy:.4f}\"\n",
    "    )\n",
    "    print(\"\\ngrammar decoding:\")\n",
    "    print(\n",
    "        f\"stim\\n\\tSFFW acc:{sffw_stim_grammar_acc:.4f}\\n\\taw2aw acc:{aw2aw_stim_grammar_acc:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"mackenzie_soukoreff\\n\\tSFFW acc:{sffw_machenzie_soukoreff_grammar_acc:.4f}\\n\\taw2aw acc:{aw2aw_machenzie_soukoreff_grammar_acc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 614312,
     "status": "aborted",
     "timestamp": 1726541846251,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "2YHPoDJpZWhL"
   },
   "outputs": [],
   "source": [
    "if EXPERIMENT_TYPE == \"subj_dependent\":\n",
    "    SVM_result_cache_path = f\"./brainbraille_SVM_result_cache_{EXPERIMENT_TYPE}_{FILE_TO_USE}_{STIMULI_LABEL_INTERVAL_s}s_sub_{sub}_test_size_{n}_{datetime.now(pytz.UTC).strftime('%m_%d_%H_%M_%S')}.bin\"\n",
    "else:\n",
    "    SVM_result_cache_path = f\"./brainbraille_SVM_result_cache_{EXPERIMENT_TYPE}_{FILE_TO_USE}_{STIMULI_LABEL_INTERVAL_s}s_test_sub_{test_sub}_{datetime.now(pytz.UTC).strftime('%m_%d_%H_%M_%S')}.bin\"\n",
    "\n",
    "\n",
    "cached_results = {\n",
    "    \"regressor_types\": regressor_types,\n",
    "    \"test_letter_label_list\": test_letter_label_list,\n",
    "    \"test_state_label_list\": test_state_label_list,\n",
    "\n",
    "    \"test_naive_prob_letter_pred_list\": test_naive_prob_letter_pred_list,\n",
    "\n",
    "    \"region_train_valid_state_label_list\": region_train_valid_state_label_list,\n",
    "    \"region_train_valid_state_pred_list\": region_train_valid_state_pred_list,\n",
    "    \"region_test_state_pred_list\": region_test_state_pred_list,\n",
    "\n",
    "    \"region_cv_score_train_list\": region_cv_score_train_list,\n",
    "    \"region_cv_score_test_list\": region_cv_score_test_list,\n",
    "\n",
    "    \"test_naive_prob_letter_test_acc_list\": test_naive_prob_letter_test_acc_list,\n",
    "\n",
    "    \"stimulus_forward_decode_letter_list\": stimulus_forward_decode_letter_list,\n",
    "    \"stimulus_forward_decode_train_acc_list\": stimulus_forward_decode_train_acc_list,\n",
    "    \"stimulus_forward_decode_test_acc_list\": stimulus_forward_decode_test_acc_list,\n",
    "    \"stimulus_viterbi_decode_letter_list\": stimulus_viterbi_decode_letter_list,\n",
    "    \"stimulus_viterbi_decode_train_acc_list\": stimulus_viterbi_decode_train_acc_list,\n",
    "    \"stimulus_viterbi_decode_test_acc_list\": stimulus_viterbi_decode_test_acc_list,\n",
    "    \"mackenzie_soukoreff_forward_decode_letter_list\": mackenzie_soukoreff_forward_decode_letter_list,\n",
    "    \"mackenzie_soukoreff_forward_decode_train_acc_list\": mackenzie_soukoreff_forward_decode_train_acc_list,\n",
    "    \"mackenzie_soukoreff_forward_decode_test_acc_list\": mackenzie_soukoreff_forward_decode_test_acc_list,\n",
    "    \"mackenzie_soukoreff_viterbi_decode_letter_list\": mackenzie_soukoreff_viterbi_decode_letter_list,\n",
    "    \"mackenzie_soukoreff_viterbi_decode_train_acc_list\": mackenzie_soukoreff_viterbi_decode_train_acc_list,\n",
    "    \"mackenzie_soukoreff_viterbi_decode_test_acc_list\": mackenzie_soukoreff_viterbi_decode_test_acc_list,\n",
    "    \"stimulus_pred_letter_viterbi_decode_letter_label_list\": stimulus_pred_letter_viterbi_decode_letter_label_list,\n",
    "    \"stimulus_pred_letter_viterbi_decode_train_acc_list\": stimulus_pred_letter_viterbi_decode_train_acc_list,\n",
    "    \"stimulus_pred_letter_viterbi_decode_test_acc_list\": stimulus_pred_letter_viterbi_decode_test_acc_list,\n",
    "    \"aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list\": aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list,\n",
    "    \"aw2aw_stimulus_pred_letter_viterbi_decode_train_acc_list\": aw2aw_stimulus_pred_letter_viterbi_decode_train_acc_list,\n",
    "    \"aw2aw_stimulus_pred_letter_viterbi_decode_test_acc_list\": aw2aw_stimulus_pred_letter_viterbi_decode_test_acc_list,\n",
    "    \"mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list\": mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list,\n",
    "    \"mackenzie_soukoreff_pred_letter_viterbi_decode_train_list\": mackenzie_soukoreff_pred_letter_viterbi_decode_train_list,\n",
    "    \"mackenzie_soukoreff_pred_letter_viterbi_decode_test_list\": mackenzie_soukoreff_pred_letter_viterbi_decode_test_list,\n",
    "    \"aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list\": aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list,\n",
    "    \"aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_train_list\": aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_train_list,\n",
    "    \"aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_test_list\": aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_test_list,\n",
    "}\n",
    "print(SVM_result_cache_path)\n",
    "write_file(\n",
    "    msgpack.packb(cached_results, use_bin_type=True), SVM_result_cache_path, \"wb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 614313,
     "status": "aborted",
     "timestamp": 1726541846253,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "jEQyxZGIEhRR"
   },
   "outputs": [],
   "source": [
    "# print(len(stimulus_forward_decode_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 614314,
     "status": "aborted",
     "timestamp": 1726541846256,
     "user": {
      "displayName": "Yuhui Zhao",
      "userId": "16322148792158071334"
     },
     "user_tz": 240
    },
    "id": "zDeSFXIbEh7V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "brainbrailleenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
